{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/antonioloison/Projects/illuin/quiz-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from tiktoken import Encoding, encoding_for_model\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nb(text: str, tokenizer: AutoTokenizer) -> int:\n",
    "    token_nb = len(tokenizer.encode(text))\n",
    "    return token_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_splits(\n",
    "#     transcripts: List[str],\n",
    "#     tokenizer: AutoTokenizer,\n",
    "#     max_length: int = 1000,\n",
    "# ):\n",
    "#     process_pile = [(transcript, \"paragraph\") for transcript in transcripts]\n",
    "#     current_transcript = \"\"\n",
    "#     splits = []\n",
    "#     counter = 0\n",
    "#     current_length = 0\n",
    "#     while len(process_pile) > 0:\n",
    "#         current_length = get_token_nb(current_transcript, tokenizer)\n",
    "\n",
    "#         if current_length > max_length:\n",
    "#             sentences = sent_tokenize(current_transcript)\n",
    "#             process_pile = [(sent, \"sentence\") for sent in sentences] + process_pile\n",
    "#             current_transcript = \"\"\n",
    "\n",
    "#         else:\n",
    "#             new_transcript = process_pile[0][0]\n",
    "#             future_transcript = \" \".join([current_transcript, new_transcript])\n",
    "#             future_length = get_token_nb(future_transcript, tokenizer)\n",
    "#             if future_length > max_length:\n",
    "#                 if process_pile[0][1] == \"paragraph\":\n",
    "#                     splits.append(current_transcript.strip())\n",
    "#                     current_transcript = new_transcript\n",
    "#                     process_pile.pop(0)\n",
    "#                 elif process_pile[0][1] == \"sentence\":\n",
    "#                     words = new_transcript.split()\n",
    "#                     process_pile = [(word, \"word\") for word in words] + process_pile[1:]\n",
    "#                 else:\n",
    "#                     splits.append(current_transcript.strip())\n",
    "#                     current_transcript = new_transcript\n",
    "#                     process_pile.pop(0)\n",
    "\n",
    "#             else:\n",
    "#                 current_transcript = future_transcript\n",
    "#                 process_pile.pop(0)\n",
    "#         counter += 1\n",
    "#         if counter > 10000:\n",
    "#             raise ValueError(\"Infinite loop\")\n",
    "\n",
    "#     if current_length > 0:\n",
    "#         splits.append(current_transcript.strip())\n",
    "\n",
    "#     return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(\n",
    "    transcripts: List[str],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_length: int = 1000,\n",
    "):\n",
    "    transcripts_to_process = []\n",
    "    max_length_tenth = max_length // 10\n",
    "    false_max_length = max_length - max_length_tenth\n",
    "    for transcript in transcripts:\n",
    "        transcript_length = get_token_nb(transcript, tokenizer)\n",
    "        if transcript_length > false_max_length:\n",
    "            sentences = sent_tokenize(transcript)\n",
    "            for sentence in sentences:\n",
    "                sentence_length = get_token_nb(sentence, tokenizer)\n",
    "                if sentence_length > false_max_length:\n",
    "                    for word in sentence.split():\n",
    "                        transcripts_to_process.append(word)\n",
    "                else:\n",
    "                    transcripts_to_process.append(sentence)\n",
    "        else:\n",
    "            transcripts_to_process.append(transcript)\n",
    "    splits = []\n",
    "    current_transcript = transcripts_to_process[0]\n",
    "    for next_transcript in transcripts_to_process[1:]:\n",
    "        future_transcript = current_transcript + \" \" + next_transcript\n",
    "        future_length = get_token_nb(future_transcript, tokenizer)\n",
    "        if future_length > false_max_length:\n",
    "            splits.append(current_transcript)\n",
    "            current_transcript = next_transcript\n",
    "        else:\n",
    "            current_transcript = future_transcript\n",
    "    if get_token_nb(current_transcript, tokenizer) < max_length_tenth:\n",
    "        splits[-1] += \" \" + current_transcript\n",
    "    else:\n",
    "        splits.append(current_transcript)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_splits(\n",
    "    [\n",
    "        \"[Music]\",\n",
    "        \"okay today we are on the last section of\",\n",
    "        \"chapter 9 section 9.3 we're still\",\n",
    "        \"talking about energy but today we're not\",\n",
    "        \"gonna talk about energy in plants we're\",\n",
    "        \"gonna talk about cellular respiration\",\n",
    "        \"we're gonna talk about how that process\",\n",
    "        \"works and I talked about photosynthesis\",\n",
    "        \"we're talking about cellular respiration\"\n",
    "    ],\n",
    "    tokenizer,\n",
    "    30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_generation.preprocessing.preprocessing import load_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts = [\n",
    "    load_txt(\"data/en/biology_high_school_class/transcript.txt\"),\n",
    "    load_txt(\"data/en/harvard_transcript/philosophy_lecture.txt\"),\n",
    "    load_txt(\"data/en/literature_class/transcript.txt\"),\n",
    "    load_txt(\"data/en/mit_videos_transcripts/clustering_transcript.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in all_transcripts:\n",
    "    splits = get_splits(\n",
    "        transcript,\n",
    "        tokenizer,\n",
    "        1000\n",
    "    )\n",
    "    print([get_token_nb(split, tokenizer) for split in splits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
